<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Boosting Viewpoint Invariance of Vision-Language Pre-training Models">
  <meta name="keywords" content="Omniview-Tuning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title> Omniview-Tuning: Boosting Viewpoint Invariance of Vision-Language Pre-training Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
/Users/panlu/Library/Mobile Documents/com~apple~CloudDocs/ImageMath/visual-mathqa-server/data_final/images
    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link rel="icon" href="./static/images/omniview-fig/logo.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <!-- <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a> -->
      <!-- @PAN TODO: consider adding links? -->
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/Heathcliff-saku/ViewFool_">
            <b>ViewFool</b> <p style="font-size:18px; display: inline; margin-left: 5px;">ðŸ”¥</p>
          </a>
          <a class="navbar-item" href="https://github.com/Heathcliff-saku/VIAT">
            <b>VIAT</b> <p style="font-size:18px; display: inline; margin-left: 5px;">ðŸ”¥</p>
          </a>
          <a class="navbar-item" href="https://github.com/Aries-iai/TT3D">
            <b>TT3D</b> <p style="font-size:18px; display: inline; margin-left: 5px;">ðŸ”¥</p>
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <img src="static/images/omniview-fig/logo.png" style="width:1.5em;vertical-align: middle" alt="Logo"/>
            <span class="Omniview-Tuning" style="vertical-align: middle">Omniview-Tuning</span>
            </h1>
          <h2 class="subtitle is-3 publication-subtitle">
            Boosting Viewpoint Invariance of Vision-Language Pre-training Models
            <!-- <br> -->
            <!-- with GPT-4V, Bard, and Other Large Multimodal Models -->
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://heathcliff-saku.github.io/">Shouwei Ruan</a><sup style="color:#6fbf73;">1</sup>,</span>
            <span class="author-block">
              <a href="https://ml.cs.tsinghua.edu.cn/~yinpeng/">Yinpeng Dong</a><sup style="color:#ed4b82;">2,</sup> <sup style="color:#ffac33">3</sup>,</span>
            <span class="author-block">
              <a href="">Hanqin Liu</a><sup style="color:#6fbf73;">1</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/Aries-iai">Yao Huang</a><sup style="color:#6fbf73;">1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.suhangss.me/">Hang Su</a><sup style="color:#ed4b82;">2</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/xingxingwei1988/">Xingxing Wei</a><sup style="color:#6fbf73;">1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup style="color:#6fbf73;">1</sup>Beihang University,</span><br>
            <span class="author-block"><sup style="color:#ed4b82">2</sup>Tsinghua University,</span>
            <span class="author-block"><sup style="color:#ffac33">3</sup>RealAI</span><br>
            </span> <img src="static/images/omniview-fig/eccv-logo.svg" style="width:5em;vertical-align: middle" alt="Logo"/></span><br>
            <span class="paper-block"> <b style="color:#f41c1c">ECCV'2024 (Oral)</b> </span>
          </div>
        
          <!-- <section> -->
            <!-- <div class="section" id="org-banners" style="display:fle">
              <a href="https://www.ucla.edu/" target="_blank" rel="external">
                  <img class="center-block org-banner" src="static/images/ucla.png" style="height:3em">
              </a>
              <a href="https://www.washington.edu/" target="blank" class="ext-link">
                  <img class="center-block org-banner" src="static/images/uw.png" style="height:3em">
              </a>
              <a href="https://www.microsoft.com/en-us/research/" target="_blank" rel="external">
                  <img class="center-block org-banner" src="static/images/microsoft.png" style="height:3em">
              </a>
            </div> -->
          <!-- </section> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <!-- @PAN TODO: change links -->
                <a href="https://arxiv.org/pdf/2404.12139"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2404.12139"
                   class="external-link button is-normal is-rounded is-dark">
                <!-- <a href="https://lupantech.github.io/papers/arxiv23_mathvista.pdf"
                   class="external-link button is-normal is-rounded is-dark"> -->
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Heathcliff-saku/Omniview_Tuning"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/RSW233/MVCap-4M"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ðŸ¤—</p>
                      <!-- ðŸ”— -->
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/RSW233/MVCap-4M"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ðŸ¤—</p>
                      <!-- ðŸ”— -->
                  </span>
                  <span>Weight</span>
                </a>
              </span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="content has-text-centered">
      <img src="static/images/tease_scores_gpt4v.png" alt="geometric reasoning" width="99%"/>
      <p> Accuracy scores of one leading LLM (i.e., PoT GPT-4), four primary LMMs, random chance, and human performance our proposed 
      <img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
      <span class="mathvista">MathVista</span>
      across mathematical reasoning and visual context types. PoT refers to program-of-thought prompting, and PoT GPT-4 is a textual LLM augmented with the caption and OCR text. GPT-4V is manually evaluated via the playground chatbot.
      </p>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container" style="margin-top: -150px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/omniview-fig/fig-1.1.png" alt="geometric reasoning" width="70%"/>
              <p> <b class="best-score-text" style="color: #7a1ae1"> The Challenge of Viewpoint Invariance in VLP.</b> We selected benchmarks representing clean distributions (ImageNet-1K, CIFAR-100), common 2D-OOD (ImageNet-V2, ImageNet-R(endition), ImageNet-Sketch), and viewpoint-OOD (ImageNet-V(iewpoint)+, OOD-CV(Pose), MIRO). <b class="best-score-text" style="color: #C6011F"> We display samples from these data distributions </b> and report the Top-1 accuracy of the original CLIP (ViT-L/14) and our improved
              <img src="static/images/omniview-fig/logo.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
              <span class="mathvista">OVT-CLIP</span> (ViT-L/14)
              </p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/omniview-fig/fig-1.2.png" alt="geometric reasoning" width="70%"/>
              <p> <b class="best-score-text" style="color: #7a1ae1"> The Challenge of Viewpoint Invariance in VLP.</b> We selected benchmarks representing clean distributions (ImageNet-1K, CIFAR-100), common 2D-OOD (ImageNet-V2, ImageNet-R(endition), ImageNet-Sketch), and viewpoint-OOD (ImageNet-V(iewpoint)+, OOD-CV(Pose), MIRO). We display samples from these data distributions and <b class="best-score-text" style="color: #C6011F"> report the Top-1 accuracy of the original CLIP (ViT-L/14) and our improved </b>
              <img src="static/images/omniview-fig/logo.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
              <span class="mathvista">OVT-CLIP</span> (ViT-L/14)
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
</section>


<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">ðŸ””News</h2>
        <div class="content has-text-justified">
          <p>
            <b>ðŸš€[2024-08-13]: Congratulations to our Omniview-Tuning for being selected for ECCV2024 Oral, see you in Milan</a>!ðŸŒŸ</b>
          </p>
          <p>
            <b>ðŸ”¥[2024-07-15]: We released the training code of Omniview-Tuning, and part of the MVCap dataset, feel free to try tuning your viewpoint-robustness VLP models!</b>
          </p>
      </div>

        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
           <b> Vision-Language Pre-training (VLP) models</b>, such as CLIP and BLIP,
            have shown great promise in learning transferable representations across 
            various vision tasks. However, a recent study (<a href="https://arxiv.org/pdf/2307.11528">Ruan et.al, 2023</a>) identifies that although VLP
            models excel at handling OOD data of 2D images, they suffer significant 
            performance degradation under 3D viewpoint changes, revealing a notable 
            shortcoming of the existing VLP models.

          </p>
          <p>
            To adress this gap,
            we build the Multi-View Caption (MVCap) dataset â€” a comprehensive
            collection of over four million multi-view image-text pairs across more
            than 100K objects, providing more potential for VLP models to develop
            generalizable viewpoint-invariant representations.
            We present <img src="static/images/omniview-fig/logo.png" style="width:1.5em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista">Omniview-Tuning</span>, a novel fine-tuning framework 
            to enhance the viewpoint invariance of VLP models. In 
            <img src="static/images/omniview-fig/logo.png" style="width:1.5em;vertical-align: middle" alt="Logo"/> <span class="mathvista">Omniview-Tuning</span>, 
            we introduce a Cross-Viewpoint Alignment objective through a minimax-like optimization strategy, effectively
            aligns representations of identical objects from diverse viewpoints without 
            causing overfitting. Additionally, OVT fine-tunes VLP models in a
            parameter-efficient manner, leading to minimal computational cost.
          </p>
          <p>
            We conduct extensive experiments to show the efficacy of the OVT framework
            in improving the viewpoint invariance for VLP models while maintaining performance on clean data and 2D-OOD samples. For example, by fine-tuning CLIP
            with OVT on different architectures (ViT-B/32, ViT-B/16, and ViT-L/14), the
            Top-1 accuracy on viewpoint-OOD benchmarks increased by an average of <b>9.6%</b>,
            <b>10.2%</b>, and <b>8.9%</b>, respectively, with only a minimal sacrifice on 
            2D-OOD benchmarks by an average of <b>2.6%</b>, <b>1.4%</b>, and <b>0.2%</b>. Furthermore, serving 
            as the visual encoder in VLLMs (e.g., LLaVa), OVT-CLIP also effectively 
            improves viewpoint invariance in image captioning and visual question answering tasks. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<section class="section">
  <div class="container" style="margin-top: -150px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m">
            <div class="content has-text-centered">
              <img src="static/images/omniview-fig/fig-2.1.png" alt="geometric reasoning" width="70%"/>
              </p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/omniview-fig/fig-2.1.png" alt="geometric reasoning" width="70%"/>
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
</section>


<!-- DATASET SECTION -->
<section class="hero is-light is-small">
  <div class="container" style="margin-bottom: 20px;"></div>
  <div class="hero-body has-text-centered">
    <!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
  <h1 class="title is-2 mathvista">
    <img src="static/images/omniview-fig/logo.png" style="width:1.5em;vertical-align: middle" alt="Logo"/>
    <span class="mathvista" style="vertical-align: middle">MultiView CAPtion Dataset</span>
  </h1>
  </div>
</section>

<!-- <section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p> -->
            
<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-full-width has-text-centered"> -->
        <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Overview</h2> -->
        <div class="content has-text-justified">
          <p>
            We introduce a large-scale <b>Multi-View Caption (MVCap) dataset</b> tailored for viewpoint invariance
            of VLP models, comprising over <b>4.6 million</b> multi-view image-text pairs across
            <b>>100K</b> objects. To assemble a diverse collection of multi-view imagetext pairs, we amalgamate various 3D assets with real-world multi-view data.
            This process involves an extensive selection and rendering of multi-view images
            from existing datasets. We then utilize a Vision Large Language Model (VLLM)
            for automated caption generation to obtain semantically rich textual descriptions without extensive manual efforts. To ensure category consistency across
            varying viewpoints in the generated captions, we implement a category-guided
            prompting strategy, which maintains accuracy in textual descriptions for different viewpoints of the same object or scene.
            <!-- a compilation of data 1) carefully examined and filtered from 28 existing VQA and MathQA datasets and 2) manually collected by us. In total, 6,141 examples were collected from 31 different datasets. -->
          </p>

          <div id="results-carousel" class="carousel results-carousel">
            <div class="box m">
              <div class="content has-text-centered">
                <img src="static/images/omniview-fig/fig-3.1.png" alt="algebraic reasoning" width="70%"/>
                <p> We create the first multi-view image caption dataset
                  by collecting multi-view samples from existing 3D object and video datasets, 
                  and generating category-guided descriptions using VLLMs.</p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/omniview-fig/fig-3.2.png" alt="arithmetic reasoning" width="70%"/>
                <p> Generated multi-view captions with common and category-guided prompts. </p>
              </div>
            </div>
          </div>

          <!-- <div class="content has-text-centered">
            <img src="static/images/source_dataset.png" alt="geometric reasoning" width="60%"/>
            <p> Summary of the 31 different source datasets in <img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
              <span class="mathvista">MathVista</span>.
            </p>
          </div>
          <div class="content has-text-centered">
            <img src="static/images/our_new_3_datasets.png" alt="geometric reasoning" width="70%"/>
            <p> Examples of our newly annotated datasets: IQTest, FunctionQA, and PaperQA.</p>
          </div> -->

          <p>
            We sampled source multi viewpoint images from three existing 3D datasets:
            <!-- @PAN TODO: add download links -->
            <ul>
              <li><b>Objavers-80k</b>: ~21K objects, ~2100K rendering viewpoint images</li>
              <li><b>IM3D</b>: 1K objects, 100K rendering viewpoint images </li>
              <li><b>MVImgNet</b>: ~ 78K objects, ~2400K real-world viewpoint images </li>
            </ul>
            You can download the dataset on <a href="https://huggingface.co/datasets/RSW233/MVCap-4M" target="_blank">Hugging Face Dataset</a>.
          </p>
        
        </div>
        
      </div>
    </div>
</section>


<!-- MRTHOD SECTION -->
<section class="hero is-light is-small">
  <div class="container" style="margin-bottom: 20px;"></div>
  <div class="hero-body has-text-centered">
    <!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
  <h1 class="title is-1 mathvista">
    <img src="static/images/omniview-fig/logo.png" style="width:1.5em;vertical-align: middle" alt="Logo"/>
    <span class="mathvista" style="vertical-align: middle">OmniView-Tuning</span>
  </h1>
  </div>
</section>

<!-- <section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p> -->
            
<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-full-width has-text-centered"> -->
        <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Overview</h2> -->
        <div class="content has-text-justified">
          <div class="content has-text-centered">
            <img src="static/images/omniview-fig/fig-4.1.png" alt="geometric reasoning" width="99%"/>
            </p>
          </div>
          <p>
            <img src="static/images/omniview-fig/logo.png" style="width:1.5em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista" style="vertical-align: middle">OmniView-Tuning</span>
          employs multi-view image-text pairs for training additional learnable components. To amplify the model's proficiency in learning viewpoint-invariant representations, we introduce a Cross-viewpoint Alignment objective, ensuring
          that representations of the same object from different viewpoints are close and
          unified in the high-dimensional feature space. </p>
          <p>To prevent performance trade-offs
          due to the concept drift from aggressive viewpoint alignment, we innovatively
          construct the optimization paradigm of OVT in a minimax-like form. The
          optimization process includes identifying extreme outlier viewpoints during the
          maximization step, while optimizing the model's invariant representation for
          these outlier samples in the minimization step. This strategy enables the model
          to focus more on the worst-case viewpoint samples, thereby maximally preserving the original embedding distribution and avoiding performance degradation
          while saving computational costs. Moreover, OVT is designed in a ParameterEfficient Fine-Tuning manner to improve efficiency, and creatively incorporates
          two trainable parameter modules: an embedding transformation module named
          VIFormer and the Low-Rank Adaptation (LoRA) weights, to acquire additional viewpoint invariance capabilities efficiently
            <!-- a compilation of data 1) carefully examined and filtered from 28 existing VQA and MathQA datasets and 2) manually collected by us. In total, 6,141 examples were collected from 31 different datasets. -->
          </p>
        
        </div>
        
      </div>
    </div>
</section>


<!-- RESULTS SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mathvista">Experiment Results</h1>
  </div>
  <div class="hero-body has-text-centered">
    <h1 class="title is-3">Results of Zero-shot Classification</h1>
    <div class="content has-text-justified">
      <div class="content has-text-centered">
        <img src="static/images/omniview-fig/fig-5.1.png" alt="geometric reasoning" width="70%"/>
        </p>
        <p> <b>Configurations of OVT and zero-shot Top-1 accuracy (%) on
          ImageNet-1K with ImageNet-V+.</b> </p> <p> The number in parentheses shows the performance change relative to the pre-trained weights. Through OVT training, each model
          maintains the performance on ImageNet-1K (IN-1K) while significantly improving the
          performance on ImageNet-V+ (IN-V+.), narrowing the performance gap. </p>
      </div>

        <div class="content has-text-justified">
          <div class="content has-text-centered">
            <img src="static/images/omniview-fig/fig-5.2.png" alt="geometric reasoning" width="70%"/>
            </p>
            <p> <b>Top-1/Top-5 zero-shot accuracy (%) under different benchmarks.</b> </p> <p>  OVT significantly enhances the models' invariance to Viewpoint-OOD samples. 
              Across different VLP architectures and visual encoders, OVT-trained models perform best on almost all viewpoint-OOD benchmarks. On the average accuracy of viewpoint-OOD datasets, OVT-OpenCLIP with ViT-B/32, ViT-B/16,
              and ViT-L/14 shows improvements of 9.6%, 10.2%, and 8.9% over OpenCLIP,
              respectively. OVT-BLIP demonstrated an average improvement of 8.6%. </p>
          </div>
      
          <div class="hero-body has-text-centered">
            <h1 class="title is-3">Results of Image Caption & VQA Tasks</h1>

              <div class="columns is-centered m-6">
                <div class="column is-full has-text-centered content">
                  <div id="results-carousel" class="carousel results-carousel">
                    <div class="box m">
                      <div class="content has-text-centered">
                        <img src="static/images/omniview-fig/fig-5.3.png" alt="geometric reasoning" width="70%"/>
                        </p> <p>The answers generated by <b style="color:#bf6fb4;">OpenFlamingo-3B</b> using our OVT-CLIP and the
                          original OpenAI CLIP as vision encoder, where red texts indicates incorrect category
                          descriptions, and green texts represents correct.</p>
                      </div>
                    </div>
                    <div class="box m-5">
                      <div class="content has-text-centered">
                        <img src="static/images/omniview-fig/fig-5.4.png" alt="geometric reasoning" width="70%"/>
                        </p> <p>The image descriptions generated by <b style="color:#6fbfac;">LLaVa-13B</b>
                          using our OVT-CLIP and the original OpenAI CLIP as
                          vision encoder, where red texts indicates incorrect category descriptions, and green texts represents correct</p>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <div class="content has-text-justified">
                <div class="content has-text-centered">
                  <img src="static/images/omniview-fig/fig-5.5.png" alt="geometric reasoning" width="70%"/>
                  </p>
                  <p> <b>Image captioning performance under clean distribution samples and
                    viewpoint-OOD samples from Real-world and Synthetic domains.</b> We utilize the MPNet to calculate the similarity between generated descriptions and ground-truth
                    labels, considering predictions successful if they exceed the similarity threshold Î².
                    </p>
                </div>
  </div>
</section>



<!-- @PAN TODO: bibtex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <p>If you find our work useful, please consider citing our paper: </p>
    <pre><code>@article{ruan2024omniview,
      title={Omniview-Tuning: Boosting Viewpoint Invariance of Vision-Language Pre-training Models},
      author={Ruan, Shouwei and Dong, Yinpeng and Liu, Hanqing and Huang, Yao and Su, Hang and Wei, Xingxing},
      journal={arXiv preprint arXiv:2404.12139},
      year={2024}
    }</code></pre>
    And welcome to to refer to our previous work in Viewpoint Robustness/Invariance studies:
    <pre><code>@inproceedings{ruan2023towards,
      title={Towards viewpoint-invariant visual recognition via adversarial training},
      author={Ruan, Shouwei and Dong, Yinpeng and Su, Hang and Peng, Jianteng and Chen, Ning and Wei, Xingxing},
      booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
      pages={4709--4719},
      year={2023}
    }</code></pre>
    <pre><code>@article{dong2022viewfool,
      title={Viewfool: Evaluating the robustness of visual recognition to adversarial viewpoints},
      author={Dong, Yinpeng and Ruan, Shouwei and Su, Hang and Kang, Caixin and Wei, Xingxing and Zhu, Jun},
      journal={Advances in Neural Information Processing Systems},
      volume={35},
      pages={36789--36803},
      year={2022}
    }</code></pre>
  </div>
</section>

<section>
  <div class="section" id="org-banners" style="display:flex">
    <a href="https://www.buaa.edu.cn" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/omniview-fig/beihang-logo.png">
    </a>
    <a href="https://www.buaa.edu.cn" target="_blank" class="external">
        <img class="center-block org-banner" src="static/images/omniview-fig/tsinghua-logo.jpg">
    </a>
    <a href="https://realai.ai/" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/omniview-fig/realai-logo.svg">
    </a>
  </div>
</section>


<footer class="footer">
  <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</footer>

</body>
</html>
